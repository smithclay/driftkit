{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### driftkit\n",
    "\n",
    "LLM-powered pyschogeographic exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --quiet -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -c conda-forge ffmpeg -y\n",
    "# restart the kernel after installing ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def navigate(\n",
    "    from_lat: float,\n",
    "    from_lon: float,\n",
    "    location_id: str,\n",
    "    config: RunnableConfig,\n",
    ") -> None:\n",
    "    \"\"\"Navigate to a location.\n",
    "\n",
    "    Args:\n",
    "        location_id: ID of the location to navigate to.\n",
    "    \"\"\"\n",
    "    navigator = config.get(\"configurable\", {}).get(\"navigator\")\n",
    "    navigation_result = navigator.navigate(from_lat, from_lon, location_id)\n",
    "    return {\n",
    "            \"tool_message\": navigation_result[\"message\"], \n",
    "            \"current_location\": navigation_result[\"current_location\"],\n",
    "            \"walking_directions\": navigation_result[\"walking_directions\"]\n",
    "            }\n",
    "\n",
    "@tool\n",
    "def get_next_possible_destinations(from_lat: float, from_lon: float, config: RunnableConfig) -> None:\n",
    "    \"\"\"Get next possible destinations.\"\"\"\n",
    "    navigator = config.get(\"configurable\", {}).get(\"navigator\")\n",
    "    result = navigator.get_next_possible_destinations(from_lat, from_lon)\n",
    "    \n",
    "    if result[\"status\"] == \"complete\":\n",
    "        return {\"tool_message\": \"Your route is complete. You have visited all possible locations.\"}\n",
    "\n",
    "    if result[\"status\"] != \"success\":\n",
    "        return {\"tool_message\": \"Failed to get next possible destinations.\"}\n",
    "    \n",
    "    destinations = result[\"destinations\"]\n",
    "    destinations_list = \"\\n\".join([f\"- {dest['name']} (ID: {dest['id']})\" for dest in destinations])\n",
    "    return {\"tool_message\": destinations_list}\n",
    "\n",
    "\n",
    "tools = [navigate, get_next_possible_destinations]\n",
    "tool_node = ToolNode(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\")\n",
    "model_with_tools = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\").bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, TypedDict, Annotated\n",
    "\n",
    "from street_navigator import Location, RouteStep\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "class MessagesStateWithLocation(MessagesState):\n",
    "    start_location: str\n",
    "    trip_summary: str\n",
    "    max_locations: int\n",
    "    current_location: Location = None\n",
    "    visited_locations: List[Location]\n",
    "    route: List[RouteStep]\n",
    "\n",
    "def begin_walk(state: MessagesStateWithLocation, config: RunnableConfig):\n",
    "    navigator = config.get(\"configurable\", {}).get(\"navigator\")\n",
    "    new_route = navigator.start_route(state[\"start_location\"])\n",
    "    current_location_id = new_route[\"current_location\"][\"id\"]\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=f\"You started your walk at {state['start_location']}.\")],\n",
    "        \"current_location\": new_route[\"current_location\"],\n",
    "        \"visited_locations\": [new_route[\"current_location\"]]\n",
    "    }\n",
    "\n",
    "def should_continue(state: MessagesStateWithLocation):\n",
    "    messages = state[\"messages\"]\n",
    "\n",
    "    if len(state[\"visited_locations\"]) >= state[\"max_locations\"]:\n",
    "        return \"summarize_journey\"\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    \n",
    "    raise Exception(\"Should continue not implemented\")\n",
    "\n",
    "    return END\n",
    "\n",
    "def update_state_from_tools(state: MessagesStateWithLocation, config: RunnableConfig):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # parse tool last_message string into dict\n",
    "    import json\n",
    "    content = json.loads(last_message.content)\n",
    "\n",
    "    if \"tool_message\" in content and \"current_location\" in content:\n",
    "        return {\n",
    "                \"messages\": [HumanMessage(content=content[\"tool_message\"])], \n",
    "                \"visited_locations\": state[\"visited_locations\"] + [content[\"current_location\"]],\n",
    "                \"current_location\": content[\"current_location\"],\n",
    "                \"route\": state[\"route\"] + content.get(\"walking_directions\", [])\n",
    "                }\n",
    "    if \"tool_message\" in content:\n",
    "        return {\"messages\": [HumanMessage(content=content[\"tool_message\"])]}\n",
    "\n",
    "\n",
    "def summarize_journey(state: MessagesStateWithLocation):\n",
    "    visited_locations = state[\"visited_locations\"]\n",
    "    messages = [HumanMessage(content=f\"You visited some cool places: {visited_locations}. Summarize your throughts and feelings about the journey in a short paragraph as a pyschogeographical report.\")]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"trip_summary\": response.content}\n",
    "\n",
    "def call_model(state: MessagesStateWithLocation):\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "builder = StateGraph(MessagesStateWithLocation)\n",
    "\n",
    "builder.add_node(\"begin_walk\", begin_walk)\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "builder.add_node(\"update_state_from_tools\", update_state_from_tools)\n",
    "builder.add_node(\"summarize_journey\", summarize_journey)\n",
    "\n",
    "builder.add_edge(START, \"begin_walk\")\n",
    "builder.add_edge(\"begin_walk\", \"agent\")\n",
    "builder.add_conditional_edges(\"agent\", should_continue, [\"tools\", \"summarize_journey\", END])\n",
    "builder.add_edge(\"summarize_journey\", END)\n",
    "builder.add_edge(\"tools\", \"update_state_from_tools\")\n",
    "builder.add_edge(\"update_state_from_tools\", \"agent\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from street_navigator import StreetNavigator\n",
    "\n",
    "navigator = StreetNavigator(location_search_radius=500)\n",
    "\n",
    "PROMPT = \"\"\"You are an urban pyschogeographer and French radical theorist associated with the Situationist International.\n",
    "Choose an interesting and inspiring route to explore, pay attention to the details.\n",
    "A novel and unexpected next destination should be selected so that you can explore the city in a new way.\n",
    "---\n",
    "ALWAYS reflect as a pedestrian and urban explorer before choosing a new destination using the following format:\n",
    "Novel destinations: [List of destinations]\n",
    "Destinations different from previous ones: [List of destinations]\n",
    "Destination that creates the most novel experience and interesting route: [Destination] [Destination ID].\n",
    "\n",
    "DO NOT offer any other information about most novel destination after selecting one.\n",
    "---\n",
    "ALWAYS get a list of possible destinations before navigating to the next location.\n",
    "\"\"\"\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=PROMPT)], \"max_locations\": 4, \"start_location\": \"1269 South Van Ness Ave, SF\", \"route\": [], \"visited_locations\": []}\n",
    "\n",
    "config = {\"configurable\": {\"navigator\": navigator, \"thread_id\": \"1\"}, \"recursion_limit\": 30}\n",
    "\n",
    "# TODO: make navigator stateless and save route in graph\n",
    "for chunk in graph.stream(\n",
    "    inputs, config, stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from route_visualizer import visualize_route\n",
    "\n",
    "trip_summary = graph.get_state(config).values[\"trip_summary\"]\n",
    "visited_locations = graph.get_state(config).values[\"visited_locations\"]\n",
    "print(trip_summary)\n",
    "\n",
    "print(visited_locations)\n",
    "\n",
    "visualize_route(graph.get_state(config).values[\"route\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from moviepy.editor import ImageSequenceClip, AudioFileClip, CompositeVideoClip, VideoFileClip\n",
    "from moviepy.config import change_settings\n",
    "from openai import OpenAI\n",
    "import time\n",
    "from typing import List, Dict\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "import subprocess\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class StreetMontageCreator:\n",
    "    def __init__(self, api_key: str = None, openai_api_key: str = None):\n",
    "        self._check_ffmpeg()\n",
    "        \n",
    "        self.api_key = api_key or os.getenv(\"GMAPS_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Google Maps API key is required\")\n",
    "            \n",
    "        openai_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not openai_key:\n",
    "            raise ValueError(\"OpenAI API key is required\")\n",
    "            \n",
    "        self.openai_client = OpenAI(api_key=openai_key)\n",
    "        self.temp_dir = \"temp_montage\"\n",
    "        os.makedirs(self.temp_dir, exist_ok=True)\n",
    "        self.DEFAULT_FPS = 30.0\n",
    "\n",
    "    def _check_ffmpeg(self):\n",
    "        try:\n",
    "            conda_ffmpeg = os.path.join(os.environ.get('CONDA_PREFIX', ''), 'bin', 'ffmpeg')\n",
    "            if os.path.exists(conda_ffmpeg):\n",
    "                change_settings({\"FFMPEG_BINARY\": conda_ffmpeg})\n",
    "                return\n",
    "            \n",
    "            result = subprocess.run(['which', 'ffmpeg'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                ffmpeg_path = result.stdout.strip()\n",
    "                change_settings({\"FFMPEG_BINARY\": ffmpeg_path})\n",
    "                return\n",
    "            \n",
    "            raise FileNotFoundError(\"ffmpeg not found\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                \"ffmpeg not found. Please install it using:\\n\"\n",
    "                \"conda: conda install ffmpeg\\n\"\n",
    "                \"ubuntu: sudo apt-get install ffmpeg\\n\"\n",
    "                \"macos: brew install ffmpeg\"\n",
    "            ) from e\n",
    "\n",
    "    def _get_street_view_image(self, lat: float, lon: float, heading: float = 0) -> Image.Image:\n",
    "        base_url = \"https://maps.googleapis.com/maps/api/streetview\"\n",
    "        params = {\n",
    "            \"size\": \"1280x720\",\n",
    "            \"location\": f\"{lat},{lon}\",\n",
    "            \"heading\": heading,\n",
    "            \"pitch\": \"0\",\n",
    "            \"outside\": \"true\",\n",
    "            \"key\": self.api_key\n",
    "        }\n",
    "        \n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            return Image.open(io.BytesIO(response.content))\n",
    "        else:\n",
    "            raise Exception(f\"Failed to fetch Street View image: {response.status_code}\")\n",
    "\n",
    "    def _create_voiceover(self, script: str, output_path: str) -> float:\n",
    "        response = self.openai_client.audio.speech.create(\n",
    "            model=\"tts-1\",\n",
    "            voice=\"onyx\",\n",
    "            response_format=\"aac\",\n",
    "            input=script\n",
    "        )\n",
    "        \n",
    "        response.stream_to_file(output_path)\n",
    "        \n",
    "        audio = AudioFileClip(output_path)\n",
    "        duration = float(audio.duration)\n",
    "        audio.close()\n",
    "        \n",
    "        return duration\n",
    "\n",
    "    def apply_psychedelic_filter(self, image: np.ndarray, intensity: float = 1.0) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply a psychedelic filter to an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image as numpy array\n",
    "            intensity: Filter intensity (0.0 to 1.0)\n",
    "            \n",
    "        Returns:\n",
    "            Filtered image as numpy array\n",
    "        \"\"\"\n",
    "        # Convert to float for processing\n",
    "        img_float = image.astype(float) / 255.0\n",
    "        \n",
    "        # Split into channels\n",
    "        b, g, r = cv2.split(img_float)\n",
    "        \n",
    "        # Apply color shifting\n",
    "        r_shift = np.roll(r, int(25 * intensity), axis=1)\n",
    "        b_shift = np.roll(b, int(-25 * intensity), axis=1)\n",
    "        \n",
    "        # Edge enhancement\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "        edges_float = edges.astype(float) / 255.0\n",
    "        edges_blurred = gaussian_filter(edges_float, sigma=2)\n",
    "        \n",
    "        # Create glowing edges\n",
    "        glow = gaussian_filter(edges_float, sigma=5)\n",
    "        \n",
    "        # Combine channels with shifted colors and glowing edges\n",
    "        result = cv2.merge([\n",
    "            np.clip(b_shift + glow * 0.3 * intensity, 0, 1),\n",
    "            np.clip(g + edges_blurred * 0.5 * intensity, 0, 1),\n",
    "            np.clip(r_shift + glow * 0.3 * intensity, 0, 1)\n",
    "        ])\n",
    "        \n",
    "        # Add slight color boost\n",
    "        result = np.clip(result * 1.2, 0, 1)\n",
    "        \n",
    "        # Convert back to uint8\n",
    "        return (result * 255).astype(np.uint8)\n",
    "\n",
    "    # Add this to the StreetMontageCreator class\n",
    "    def _apply_psychedelic_effects(self, images: List[np.ndarray], fps: float) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply psychedelic effects to a sequence of images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of input images\n",
    "            fps: Frames per second\n",
    "            \n",
    "        Returns:\n",
    "            List of processed images\n",
    "        \"\"\"\n",
    "        processed_images = []\n",
    "        frame_count = len(images)\n",
    "        \n",
    "        for i, image in enumerate(images):\n",
    "            # Calculate pulsing intensity based on frame position\n",
    "            time_position = i / fps\n",
    "            intensity = 0.5 + 0.3 * np.sin(2 * np.pi * time_position)\n",
    "            \n",
    "            # Apply the psychedelic filter\n",
    "            processed = self.apply_psychedelic_filter(image, intensity)\n",
    "            processed_images.append(processed)\n",
    "            \n",
    "        return processed_images\n",
    "\n",
    "    def create_montage(self, locations: List[Dict], script: str, output_path: str, fps: float = None) -> None:\n",
    "        fps = float(fps) if fps is not None else float(self.DEFAULT_FPS)\n",
    "        print(f\"Starting video creation with fps: {fps}\")\n",
    "        \n",
    "        if not locations:\n",
    "            raise ValueError(\"At least one location is required\")\n",
    "\n",
    "        # Create temporary files\n",
    "        temp_video = os.path.join(self.temp_dir, \"temp_video.mp4\")\n",
    "        audio_path = os.path.join(self.temp_dir, \"voiceover.aac\")\n",
    "        \n",
    "        try:\n",
    "            # Generate voiceover first\n",
    "            duration = self._create_voiceover(script, audio_path)\n",
    "            print(f\"Generated voiceover with duration: {duration}\")\n",
    "\n",
    "            # Calculate frames needed\n",
    "            total_frames = int(math.ceil(duration * fps))\n",
    "            frames_per_location = max(1, total_frames // len(locations))\n",
    "            print(f\"Total frames needed: {total_frames}, frames per location: {frames_per_location}\")\n",
    "\n",
    "            # Generate images\n",
    "            images = []\n",
    "            for location in locations:\n",
    "                lat = None\n",
    "                lon = None\n",
    "\n",
    "                if 'lat' in location['coordinates'] and 'lon' in location['coordinates']:\n",
    "                    lat = location['coordinates']['lat']\n",
    "                    lon = location['coordinates']['lon']\n",
    "                \n",
    "                if 'latitude' in location['coordinates'] and 'longitude' in location['coordinates']:\n",
    "                    lat = location['coordinates']['latitude']\n",
    "                    lon = location['coordinates']['longitude']\n",
    "\n",
    "                if lat is None or lon is None:\n",
    "                    raise ValueError(\"Latitude and longitude are required for each location\")\n",
    "\n",
    "                for frame in range(frames_per_location):\n",
    "                    heading = (360 / frames_per_location) * frame\n",
    "                    try:\n",
    "                        image = self._get_street_view_image(lat, lon, heading)\n",
    "                        images.append(np.array(image))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error fetching image for location {location['name']}: {e}\")\n",
    "                        if images:\n",
    "                            images.append(images[-1].copy())\n",
    "                        else:\n",
    "                            images.append(np.zeros((720, 1280, 3), dtype=np.uint8))\n",
    "                    time.sleep(0.1)\n",
    "\n",
    "            # Ensure we have enough frames\n",
    "            while len(images) < total_frames:\n",
    "                images.append(images[-1].copy())\n",
    "            images = images[:total_frames]\n",
    "            \n",
    "            print(f\"Generated {len(images)} images\")\n",
    "\n",
    "            # Apply psychedelic effects\n",
    "            print(\"Applying psychedelic effects...\")\n",
    "            images = self._apply_psychedelic_effects(images, fps)\n",
    "\n",
    "            # Create video without audio first\n",
    "            print(\"Creating video clip...\")\n",
    "            video = ImageSequenceClip(images, fps=fps)\n",
    "            if not hasattr(video, 'fps') or video.fps is None:\n",
    "                video.fps = fps\n",
    "            print(f\"Video clip created with fps: {video.fps}\")\n",
    "\n",
    "            # Write temporary video file\n",
    "            print(\"Writing temporary video file...\")\n",
    "            video.write_videofile(\n",
    "                temp_video,\n",
    "                fps=fps,\n",
    "                codec='libx264',\n",
    "                audio=False,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            # Load the video and audio separately\n",
    "            print(\"Loading video and audio for final composition...\")\n",
    "            video_clip = VideoFileClip(temp_video)\n",
    "            audio_clip = AudioFileClip(audio_path)\n",
    "\n",
    "            # Set the video duration to match the audio\n",
    "            video_clip = video_clip.set_duration(audio_clip.duration)\n",
    "\n",
    "            # Combine video and audio\n",
    "            print(\"Creating final video with audio...\")\n",
    "            final_clip = video_clip.set_audio(audio_clip)\n",
    "            \n",
    "            # Write final video\n",
    "            print(f\"Writing final video to {output_path}...\")\n",
    "            final_clip.write_videofile(\n",
    "                output_path,\n",
    "                fps=fps,\n",
    "                codec='libx264',\n",
    "                audio_codec='aac',\n",
    "                temp_audiofile=os.path.join(self.temp_dir, \"temp_audio.aac\"),\n",
    "                remove_temp=True,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            print(\"Video creation completed successfully!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error creating video: {str(e)}\")\n",
    "            \n",
    "        finally:\n",
    "            # Cleanup\n",
    "            for file in [temp_video, audio_path]:\n",
    "                if os.path.exists(file):\n",
    "                    try:\n",
    "                        os.remove(file)\n",
    "                    except:\n",
    "                        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator = StreetMontageCreator()\n",
    "creator.create_montage(\n",
    "    locations=visited_locations,\n",
    "    script=trip_summary,\n",
    "    output_path=\"street_montage.mp4\",\n",
    "    fps=3.0\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
